{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRj3mE-woslO"
   },
   "source": [
    "# Text Processing \n",
    "\n",
    "We will work with unstructured data.\n",
    "In particular, we will work with text data to build a language model (LM). \n",
    "\n",
    "LM is a probability distribution $p(\\cdot)$ over sequence of words.\n",
    "For example, given text $T=(w_1\\ w_2\\ \\dots \\ w_{|T|})$, where $w_1,\\ w_2\\, \\dots \\ w_{|T|}$ is sequence of words and $|T|$ is the number of words in the sequence, LM assigns it probability as follows:\n",
    "$p(T)=p(w_1\\ w_2\\ \\dots \\ w_{|T|})$.\n",
    "\n",
    "LM is employed in many applications such as automatic speech recognition (ASR), machine translation (MT), image captioning, text generation and so on. For example, LM is used in ASR to assess correctness of generated output transcripts.\n",
    "\n",
    "Let's assume our ASR system generated two possible transcripts for some given speech signal: \"recognize speech\" and \"wreck a nice beach\".\n",
    "These two phrases sound similar, but the first one is more likely to be linguistically correct.\n",
    "We can use the probability of each transcript to determine the final output of ASR.\n",
    "The probabilistic model (LM) should assign a higher probability score to the correct answer:\n",
    "\n",
    "$p(``\\text{recognize speech}\") > p(``\\text{wreck a nice beach}\")$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eHXr0-sP1dl"
   },
   "source": [
    "## Reading a text file\n",
    "We will use Penn Tree Bank ([PTB](https://catalog.ldc.upenn.edu/LDC99T42)) dataset as our text data in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "QDXB-a8nOU0Z",
    "outputId": "d4fa3ff1-bf4f-4097-d082-f32806918b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'str'>\n",
      "Size: 5101618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a file into the string\n",
    "with open('ptb.train.txt') as f:\n",
    "   raw_data = f.read()\n",
    "\n",
    "print(\"Type:\",type(raw_data))     # type of the data\n",
    "print(\"Size:\",str(len(raw_data))) # number of characters\n",
    "raw_data[965:1156]                # print a small snippet of the data (\"\\n\" is a symbol for new line, it indicates the sentence boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF0q7uNEX-iP"
   },
   "source": [
    "## Text preprocessing\n",
    "Take a look at the PTB dataset. It is already pre-processed, i.e. words are tokenized, all letters are lowercased, numbers are replaced with the \"N\", some non-alphanumeric characters are removed and etc. We need to apply a few small changes for our assignment. Please note, only the most frequent 9,999 words are kept in the dataset, the rest of the words are replaced with the special <\"unk>\" symbol. This is done to reduce memory and computation requirements.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEapMVB4tPWO"
   },
   "source": [
    "The PTB dataset has been pre-processed. Nevertheles, let's learn some basic text pre-processing commands.\n",
    "\n",
    "To lowercase a given string, you can use `str.lower()` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ39OpHzxjaz"
   },
   "source": [
    "To filter out the non-alphanumeric characters (e.g. comma(,), question mark(?), colon(:), dash(-), dollar sign($), equal symbol(=), plus sign(+), apostrophes(') and etc.) and split the text into the words, you can use python regular expression operations (check the [python documentation](https://docs.python.org/3/library/re.html) to learn more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "hy6VmkadXFb1",
    "outputId": "71b66516-5bed-4f4e-dbae-45441af59144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'list'>\n",
      "Size:  42068\n",
      "Sentence: mr. <unk> is chairman of <unk> n.v. the dutch publishing group\n"
     ]
    }
   ],
   "source": [
    "# Split the raw data into sentences and save them into a list (delimiter for sentence boundaries: '\\n')\n",
    "data = raw_data.split('\\n')\n",
    "#data = re.split('\\n', raw_data)  # you can also use regular expressions\n",
    "data = [sent.strip() for sent in data if sent != '']  # remove empty sentences, see \"list comprehension\" \n",
    "\n",
    "print(\"Type: \", type(data))   # type of the data\n",
    "print(\"Size: \", len(data))    # number of sentences\n",
    "print(\"Sentence: \" + data[2]) # print a sentence, try to print another sentence (symbol '<unk>' corresponds to unknown word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U0G40kJYR4i"
   },
   "source": [
    "## Word counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4-ONRCXo38W"
   },
   "source": [
    "We will create a dictionary which contains a word as a key and its frequency as a value, eg. 'judge': 262\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5ugtyrCeH0R0"
   },
   "outputs": [],
   "source": [
    "dictionary = dict()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZgfJJm6XCQs-"
   },
   "outputs": [],
   "source": [
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word in dictionary:\n",
    "            dictionary[word] += 1\n",
    "        else:\n",
    "            dictionary[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Te95O6sgCv4K"
   },
   "outputs": [],
   "source": [
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        dictionary[word] = dictionary.get(word,0) +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "1AhrqU_fXLO7",
    "outputId": "717e39b8-6e52-49f5-f213-dd5d9fab79a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 9999\n",
      "524\n",
      "90040\n"
     ]
    }
   ],
   "source": [
    "# Check the dictionary\n",
    "print(\"Size: \"+str(len(dictionary)))  # number of unique words\n",
    "print(dictionary['judge'])            # try other words\n",
    "print(dictionary['<unk>'])            # try other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Mxj1hjseKMp_"
   },
   "outputs": [],
   "source": [
    "# #Solution1\n",
    "# dictionary = dict()\n",
    "# for sent in data:\n",
    "#     for token in sent.split():\n",
    "#         if token not in dictionary:\n",
    "#             dictionary[token] = 1\n",
    "#         else:\n",
    "#             dictionary[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fdMXefhyAVHB"
   },
   "outputs": [],
   "source": [
    "#Solution2\n",
    "dictionary = dict()\n",
    "for sent in data:\n",
    "    for token in sent.split():\n",
    "        dictionary[token] = dictionary.get(token,0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K14WGWlfPMBc"
   },
   "source": [
    "## Building Unigram Language Model (probabilistic model)\n",
    "\n",
    "To build a probabilistic model of a text, we can employ the frequentist approach, i.e. use relative frequency to estimate probability scores.\n",
    "Suppose we want to estimate probability of a sentence \"how are you\", then we need to count how many times it appears in the dataset. This approach is infeasible for long and/or complex sentences that might be absent or appear a few times in the dataset.\n",
    "\n",
    "Computing probability of short phrase is easy (click [here](https://books.google.com/ngrams/graph?content=How+are+you&year_start=1800&year_end=2000&corpus=15&smoothing=3&share=&direct_url=t1%3B%2CHow%20are%20you%3B%2Cc0#t1%3B%2CHow%20are%20you%3B%2Cc0)).\n",
    "However, for a longer sentence it is difficult (click [here](https://books.google.com/ngrams/graph?content=I+met+my+friends+Madina+and+Yerbolat&year_start=1800&year_end=2000&corpus=15&smoothing=3&direct_url=))\n",
    "\n",
    "#### Review: [Chain rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))\n",
    "To circumvent the problem mentioned above, we will employ the chain rule to break down the joint probability of all words in a sentence into the sequence of conditional probabilities as follows:\n",
    "\n",
    "$p(T)=p(w_1\\ w_2\\ \\dots\\ w_{|T|}) = p(w_1)\\ p(w_2 | w_1)\\ p(w_3 | w_1\\ w_2)\\ \\dots \\ p(w_{|T|} | w_1 \\dots w_{|T|-1}) = p(w_1)\\prod^{|T|}_{i=2}p(w_i|w_1\\ \\dots\\ w_{i-1})$\n",
    "\n",
    "where $p(w_3 | w_1\\ w_2)$ - is the probability of word $w_3$ given that the preceeding two words in the sentence were $w_1$ and $w_2$.\n",
    "\n",
    "For example, if $T= ``How\\ are\\ you\"$, then $w_1=``How\"$, $w_2 = ``are\"$, and $w_3 = ``you\"$, and the probability of having such sentence is\n",
    "$p(``How\\ are\\ you\")= p(``How\")\\ p(``are\"| ``How\")\\ p(``you\" | ``How\" \\ ``are\")$\n",
    "\n",
    "The conditional probabilities are estimated as follows:\n",
    "\n",
    "$p(w_i|w_1\\ \\dots\\ w_{i-1})=\\frac{count(w_{1}\\ w_{2}\\ \\dots \\ w_{i})}{count(w_1\\ w_2\\ \\dots\\ w_{i-1})}$\n",
    "\n",
    "Nevertheless, this is still infeasible for long sentences, because computing conditional probability of the last words still requires to count occurences of the preceeding long phrases.\n",
    "\n",
    "The simplest way to solve this problem is to treat each word independent from each other ([Independence](https://en.wikipedia.org/wiki/Markov_property)). It is also known as [unigram language model](https://en.wikipedia.org/wiki/Language_model#Unigram):\n",
    "\n",
    "$p(T)=p(w_1\\ w_2\\ \\dots\\ w_{|T|}) \\approx p(w_1)\\ p(w_2)\\ p(w_3)\\ \\dots \\ p(w_{|T|}) = \\prod^{|T|}_{i=1}p(w_i)$\n",
    "\n",
    "Then: $p(``How\\ are\\ you\")\\approx p(``How\")\\ p(``are\")\\ p(``you\")$\n",
    "\n",
    "The marginal probability of each word can be estimated as follows:\n",
    "\n",
    "$p(w_i)=\\frac{count(w_i)}{\\sum count(w)} = \\frac{count(w_i)}{Total\\ number\\ of\\ words}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1hyePfXQUzQ"
   },
   "source": [
    "## Estimating probability of a word\n",
    "We will create a dictionary called **dictionary_prob** which holds the marginal probability of each word, eg. 'judge': 0.0002696424767176071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ealqz6086ow5"
   },
   "outputs": [],
   "source": [
    "total_words = sum(dictionary.values())\n",
    "dictionary_prob = dictionary.copy()\n",
    "\n",
    "for word in dictionary:\n",
    "    dictionary_prob[word] /= total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "jDcPg3UADl5l",
    "outputId": "165ad989-51d4-4aaa-c957-3161826ffd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:  887521\n",
      "Probability of 'judge':  0.0002952042824902171\n",
      "Probability of '<unk>':  0.050725560296601434\n"
     ]
    }
   ],
   "source": [
    "# check the obtained dictionary_prob\n",
    "print(\"Total number of words: \", sum(dictionary.values()))\n",
    "print(\"Probability of 'judge': \", dictionary_prob['judge'])\n",
    "print(\"Probability of '<unk>': \", dictionary_prob['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "0wdYbilLTDRb",
    "outputId": "09ff06db-eb82-4bbf-d20a-5de7605927fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887521\n",
      "262\n",
      "0.0002952042824902171\n"
     ]
    }
   ],
   "source": [
    "num_of_words = sum(dictionary.values())   # total number of words\n",
    "dictionary_prob = dictionary.copy()\n",
    "for key in dictionary_prob:\n",
    "      dictionary_prob[key] /= num_of_words\n",
    "\n",
    "print(num_of_words)\n",
    "print(dictionary['judge'])\n",
    "print(dictionary_prob['judge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eatPD24cS_lC"
   },
   "source": [
    "## Estimating probability of a sentence\n",
    "We will write a function called **compute_prob0** which takes a sentence and dictionary of marginal probabilites as input and returns its probability assuming each word is independent. For the words not present in the dictionary, probability of \"'\\<\"unk>' symbol must be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KAbDeDy6TIbh"
   },
   "outputs": [],
   "source": [
    "def compute_prob0(sentence, dictionary_prob):\n",
    "    tokens = sentence.split()\n",
    "    result = 1\n",
    "    for token in tokens:\n",
    "        if token in dictionary_prob:\n",
    "            result *= dictionary_prob[token]\n",
    "        else:\n",
    "            result *= dictionary_prob[\"<unk>\"]\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "iR9RKJIKTyEk",
    "outputId": "8fa8c010-fc45-48c4-9dad-9c074c3fdb0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris is a capital of france\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4865562113295714e-17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check you solution\n",
    "sentence=\"paris is a capital of france\"\n",
    "print(sentence)\n",
    "compute_prob0(sentence, dictionary_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr. is chairman of n.v. the dutch publishing group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.579782658636421e-27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check you solution\n",
    "sentence=\"mr. is chairman of n.v. the dutch publishing group\"\n",
    "print(sentence)\n",
    "compute_prob0(sentence, dictionary_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QN3jl7HK7_Tg"
   },
   "outputs": [],
   "source": [
    "def compute_prob0(sentence, dictionary_prob):\n",
    "    tokens = sentence.split()\n",
    "    result = 1.0\n",
    "    for token in tokens:\n",
    "        if token not in dictionary_prob:\n",
    "            result *= dictionary_prob['<unk>']\n",
    "        else:\n",
    "            result *= dictionary_prob[token]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jWN-Ta0oTVhQ"
   },
   "outputs": [],
   "source": [
    "# #Solution2\n",
    "# import numpy as np\n",
    "# def compute_prob0(sentence, dictionary_prob):\n",
    "#     tokens = sentence.split()\n",
    "#     result = []\n",
    "#     for token in tokens:\n",
    "#         if token not in dictionary_prob:\n",
    "#             result.append(dictionary_prob['<unk>'])\n",
    "#         else:\n",
    "#             result.append(dictionary_prob[token])\n",
    "#     return np.prod(np.array(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have seen how to compute joint probability of a sentence using the independence assumption.\n",
    "However, this assumption is too weak as words do depend on each other. In the second part, we would like to introduce a better approach using Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov chain**\n",
    "\n",
    "[Markov chain](https://en.wikipedia.org/wiki/Markov_chain)\n",
    " is based on the following assumption: *given the present, the future does not depend on the past*.\n",
    "\n",
    "*Zeroth order Markov chain:*\n",
    "\n",
    "The simplest form of Markov chain is so called zeroth order Markov chain where words are completely independent from each other:\n",
    "\n",
    "$p(w_1\\ w_2\\ \\dots\\ w_{|T|}) \\approx p(w_1)\\ p(w_2)\\ p(w_3)\\ \\dots \\ p(w_{|T|}) = \\prod^{|T|}_{i=1}p(w_i)$\n",
    "\n",
    "Recall that this is similar to the independence assumption covered in the first part and it is known as unigram language model.\n",
    "\n",
    "*First order Markov chain:*\n",
    "\n",
    "In the first order Markov chain, a probability of a word is conditioned only on the preceeding word:\n",
    "\n",
    "$p(w_1\\ w_2\\ \\dots\\ w_{|T|}) \\approx p(w_1)\\ p(w_2|w_1)\\ p(w_2|w_3)\\ \\dots \\ p(w_{|T|}|w_{|T|-1}) = p(w_1)\\prod^{S}_{i=2}p(w_i|w_{i-1})$\n",
    "\n",
    "This model is known as bigram (or 2-gram) language model. The conditional probabilities are estimated as:\n",
    "\n",
    "$p(w_i|w_{i-1})=\\frac{count(w_{i-1}\\ w_{i})}{count(w_{i-1})}$\n",
    "\n",
    "Similarly, in the N-th order Markov chain, a probability of a word is conditioned only on the preceeding N words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing probability of a sentence (Bigram Language Model)\n",
    "We will write a function called **compute_prob1** which takes a sentence as input (and maybe some additional arguments) and returns its probability estimated using the first order Markov chain. \n",
    "\n",
    "Different from the unigram language model, your function should first append *\\<bos\\>* and *\\<eos\\>* symbols to the input sentence and replace all words that are not present in the dataset with *\\<unk\\>* symbol. \n",
    "\n",
    "If the original sentence was \"my name is madina\", then \"madina\" is obviously not in the dataset (see the description of the PTB). The probability of such sentence can be computed as:\n",
    "\n",
    "p(\"my name is madina\") $\\approx$ p(\"my\" | \"\\<bos\\>\") * p(\"name\" | \"my\") * p(\"is\" | \"name\") * p(\"\\<\"unk\\>\" | \"is\") * p(\"\\<eos\\>\" | \"\\<\"unk\\>\")\n",
    "\n",
    "Notice that the probability of the \"\\<bos\\>\" symbol is not used during the sentence probability estimation. It is appeneded to enable the probability computation of the first word, i.e. p(\"my\" | \"\\<bos\\>\"). On the other hand, we must include the probability of \"\\<eos\\>\" symbol to obtain a valid probability distribution, i.e. the sum of the probabilities of all outcomes must equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write you code here\n",
    "dict1 = dict()\n",
    "for i in range(len(data)):\n",
    "    data[i] = '<a> ' + data[i] + ' <b>'\n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word in dict1:\n",
    "            dict1[word] += 1\n",
    "        else:\n",
    "            dict1[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramlist=[]\n",
    "for sent in data:\n",
    "    new = sent.split()\n",
    "    for word in range(len(new)-1):\n",
    "        bigramlist.append((new[word], new[word+1]))\n",
    "dict2 = dict()\n",
    "for i in bigramlist:\n",
    "    if i in dict2:\n",
    "        dict2[i] += 1\n",
    "    else:\n",
    "        dict2[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob1(sent):\n",
    "    list_ = [\"<a>\"]\n",
    "    dict_bigramLM = dict()\n",
    "    result = 1\n",
    "    for i in sent.split():\n",
    "        if i not in dict1:\n",
    "            i=\"<unk>\"\n",
    "        list_.append(i)\n",
    "    list_.append(\"<b>\")\n",
    "    print(list_)\n",
    "    \n",
    "    ll = len(list_)\n",
    "      \n",
    "    for i in list(zip(list_, list_[1:])):\n",
    "        dict_bigramLM[i] = dict2[i] / dict1[i[0]]\n",
    "        \n",
    "    for i in dict_bigramLM:\n",
    "        result *= dict_bigramLM[i]\n",
    "        \n",
    "    return dict_bigramLM.values(), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<a>', 'my', 'name', 'is', '<unk>', '<b>']\n",
      "(dict_values([0.0008082152705144053, 0.0038461538461538464, 0.06766917293233082, 0.05479078642496933, 0.07385606397156819]), 8.512130344833542e-10)\n",
      "['<a>', '<unk>', 'how', 'are', 'you', '<b>']\n",
      "(dict_values([0.03986402966625464, 0.000444247001332741, 0.002506265664160401, 0.0007664793050587634, 0.020253164556962026]), 6.89010961921847e-13)\n",
      "['<a>', 'this', 'is', 'an', 'estimate', '<b>']\n",
      "(dict_values([0.009532185984596368, 0.0808039376538146, 0.01362954886193267, 0.0002876042565429968, 0.14634146341463414]), 4.418442586905126e-10)\n"
     ]
    }
   ],
   "source": [
    "#If you completed the task correctly then your function should compute:\n",
    "sent1 = \"my name is madina\"\n",
    "print(compute_prob1(sent1))\n",
    "sent2 = \"hello how are you\"\n",
    "print(compute_prob1(sent2))\n",
    "sent3 = \"this is an estimate\"\n",
    "print(compute_prob1(sent3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Processing_Part1_OCT2021_solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
